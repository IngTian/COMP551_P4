{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFZyaMVV3Zoh"
   },
   "source": [
    "# Initialization\n",
    "\n",
    "---\n",
    "\n",
    "As mentioned in the project `README.md`, to run this project you have to upload the **zipped** project to colab, under the `/content` folder. We named the zipped file as `COMP551_P4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CExhL8f_1DoV",
    "outputId": "4c2b7c74-0df2-4b39-92df-c5ac32bfdc4a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'COMP551_P4'...\n",
      "remote: Enumerating objects: 71, done.\u001B[K\n",
      "remote: Counting objects: 100% (31/31), done.\u001B[K\n",
      "remote: Compressing objects: 100% (22/22), done.\u001B[K\n",
      "remote: Total 71 (delta 16), reused 13 (delta 9), pack-reused 40\u001B[K\n",
      "Unpacking objects: 100% (71/71), done.\n",
      "/content/COMP551_P4\n"
     ]
    }
   ],
   "source": [
    "!unzip COMP551_P4\n",
    "%cd COMP551_P4"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The use of Google Drive\n",
    "\n",
    "Though this step is not strictly necessary, it will be much easier to save results to your Google Drive in case of disconnections."
   ],
   "metadata": {
    "id": "4pL3D1D3I7ia"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hyOZD73c5QkH",
    "outputId": "fa6e796f-d45b-48fd-880c-2d7338e37acf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLdDphzI3i-t"
   },
   "source": [
    "# Run Program\n",
    "\n",
    "---\n",
    "\n",
    "At this step, you have to make sure your working directory is like this.\n",
    "```text\n",
    ".\n",
    "├── README.md\n",
    "├── acon.py\n",
    "├── classifier\n",
    "│   ├── __init__.py\n",
    "│   ├── metric.py\n",
    "│   ├── network\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── alex_net.py\n",
    "│   │   ├── resnet_acon.py\n",
    "│   │   ├── resnet_metaacon.py\n",
    "│   │   ├── resnet_relu.py\n",
    "│   │   ├── shuffle.py\n",
    "│   │   ├── shuffle_acon.py\n",
    "│   │   ├── shuffle_metaacon.py\n",
    "│   │   ├── vgg16_6_acon.py\n",
    "│   │   ├── vgg16_6_metaacon.py\n",
    "│   │   ├── vgg16_6_relu.py\n",
    "│   │   ├── vgg16_acon.py\n",
    "│   │   ├── vgg16_metaacon.py\n",
    "│   │   └── vgg16_relu.py\n",
    "│   └── plugin.py\n",
    "├── data\n",
    "│   └── __init__.py\n",
    "├── main.py\n",
    "└── p4.ipynb\n",
    "```\n",
    "To run the project, simply run `main.py`.\n",
    "\n",
    "**NOTICE**\n",
    "\n",
    "In `main.py`, the variable `TRAINED_MODELS_PATH` controls where to store partial results while training. At this stage, if you have your Google Drive connected, you may try a relative path like `../drive/MyDrive/Colab Notebooks/COMP551/Project 4/vgg_exp_results`. If you leave it as `trained-models`, it shall save partial results inside the Colab runtime, and it may gets lost due to frequent disconenctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVN5AwIn3ifl",
    "outputId": "9ba97d29-d3db-4007-c827-b5c3f79aa537"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./dataset/cifar-100-python.tar.gz\n",
      "169001984it [00:02, 69121258.79it/s]                   \n",
      "Extracting ./dataset/cifar-100-python.tar.gz to ./dataset\n",
      "tcmalloc: large alloc 8847360000 bytes == 0x5651f51fa000 @  0x7fd165e8fb6b 0x7fd165eaf379 0x7fd0656d4cde 0x7fd0656d6452 0x7fd0b76fdcf3 0x7fd0b76fe867 0x7fd0b7a51cc1 0x7fd0b7793571 0x7fd0b7793da5 0x7fd0b7e5fe52 0x7fd0b7cfd1db 0x7fd0b779cebb 0x7fd0b7f54b02 0x7fd0b7a40dfe 0x7fd0b7795769 0x7fd0b7f55772 0x7fd0b7b03042 0x7fd0b942e6c0 0x7fd0b942ee25 0x7fd0b7b583fb 0x7fd160da7ab3 0x564fd5175544 0x564fd5175240 0x564fd51e9627 0x564fd5176afa 0x564fd51e4915 0x564fd51e39ee 0x564fd5176bda 0x564fd51e5737 0x564fd51e39ee 0x564fd51e36f3\n",
      "alex-metaacon\n",
      "<class 'classifier.network.alex_net.AlexNet'>\n",
      "{'activation': 'metaacon', 'num_classes': 100, 'sizes': (64, 192, 384, 256, 256, 4096)}\n",
      "2518728\n",
      "Epochs to train: 50\n",
      "Continue from epoch: 0\n",
      "Model Summary:\n",
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): MetaAconC(\n",
      "      (fc1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): MetaAconC(\n",
      "      (fc1): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc2): Conv2d(16, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): MetaAconC(\n",
      "      (fc1): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc2): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): MetaAconC(\n",
      "      (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): MetaAconC(\n",
      "      (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=100, bias=True)\n",
      "  )\n",
      ")\n",
      "Device used for training: cuda:0\n",
      "Size of training set: 45000\n",
      "Size of validation set: 5000\n",
      "\n",
      "---1 EPOCHS FINISHED---\n",
      "Plugin messages for epoch 1:\n",
      "TRAIN: 0.1912\tVAL: 0.1811764705882353\n",
      "time elapsed: 86.73323337 sec\n",
      "---2 EPOCHS FINISHED---\n",
      "Plugin messages for epoch 2:\n",
      "TRAIN: 0.30677777777777776\tVAL: 0.27215686274509804\n",
      "time elapsed: 86.35948277900002 sec\n",
      "---3 EPOCHS FINISHED---\n",
      "Plugin messages for epoch 3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\", line 363, in __getitem__\n",
      "    return self.dataset[self.indices[idx]]\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\", line 363, in __getitem__\n",
      "    return self.dataset[self.indices[idx]]\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/datasets/cifar.py\", line 121, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n",
      "    img = t(img)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\", line 98, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\", line 148, in to_tensor\n",
      "    img = img.permute((2, 0, 1)).contiguous()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 277, in <module>\n",
      "    experiment('CIFAR100', 50)\n",
      "  File \"main.py\", line 248, in experiment\n",
      "    train_and_test(*p)\n",
      "  File \"main.py\", line 171, in train_and_test\n",
      "    train_model(model, fname, model_params, epochs, continue_from, batch_size)\n",
      "  File \"main.py\", line 131, in train_model\n",
      "    start_epoch=continue_from + 1\n",
      "  File \"/content/COMP551_P4/classifier/__init__.py\", line 186, in train\n",
      "    plugin(self, epoch)\n",
      "  File \"/content/COMP551_P4/classifier/plugin.py\", line 107, in plugin\n",
      "    clf.train_performance(metric, batch_size),\n",
      "  File \"/content/COMP551_P4/classifier/__init__.py\", line 56, in train_performance\n",
      "    return metric(self, loader)\n",
      "  File \"/content/COMP551_P4/classifier/metric.py\", line 7, in accuracy\n",
      "    for i, data in enumerate(data_loader, 0):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
      "    data = self._next_data()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "p4.ipynb",
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}